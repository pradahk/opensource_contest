import React, { useState, useEffect, useRef } from 'react';
import { useParams, useNavigate } from 'react-router-dom';
import { aiInterviewAPI } from '../services/api';
import '../styles/AIInterviewRoom.css';

const AIInterviewRoom = () => {
  const { sessionId } = useParams();
  const navigate = useNavigate();
  const [session, setSession] = useState(null);
  const [currentQuestion, setCurrentQuestion] = useState(null);
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [transcription, setTranscription] = useState('');
  const [error, setError] = useState(null);
  const [loading, setLoading] = useState(true);
  const [threadId, setThreadId] = useState(null);
  const [aiResponse, setAiResponse] = useState('');
  const [aiAudioUrl, setAiAudioUrl] = useState(null);
  const [isInitialized, setIsInitialized] = useState(false);
  
  const mediaRecorderRef = useRef(null);
  const audioChunksRef = useRef([]);
  const recognitionRef = useRef(null);

  useEffect(() => {
    if (sessionId && !isInitialized) {
      setIsInitialized(true);
      initializeSession();
      initializeSpeechRecognition();
    }
  }, [sessionId, isInitialized]);

  const initializeSession = async () => {
    try {
      setLoading(true);
      setError(null);
      console.log('AI ì¸í„°ë·° ì„¸ì…˜ ì´ˆê¸°í™” ì‹œì‘');
      
      // AI ì¸í„°ë·° ì„¸ì…˜ ì‹œì‘
      const response = await aiInterviewAPI.startSession({
        userId: localStorage.getItem('user') ? JSON.parse(localStorage.getItem('user')).id : null,
        companyId: sessionId, // sessionIdë¥¼ companyIdë¡œ ì‚¬ìš©
        position: 'ê°œë°œì' // ê¸°ë³¸ê°’ ì„¤ì •
      });
      
      console.log('AI ì¸í„°ë·° ì„¸ì…˜ ì‘ë‹µ:', response);
      
      if (response.success) {
        const newThreadId = response.thread_id;
        setThreadId(newThreadId);
        setSession({ id: sessionId, thread_id: newThreadId });
        
        // ì´ˆê¸° ì§ˆë¬¸ ìƒì„± (threadIdë¥¼ ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬)
        await generateInitialQuestion(newThreadId);
      } else {
        throw new Error(response.message || 'ì„¸ì…˜ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
      }
    } catch (err) {
      console.error('Error initializing session:', err);
      
      if (err.response) {
        const errorMessage = err.response.data?.message || err.response.data?.error || 'ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.';
        setError(`ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: ${errorMessage}`);
      } else if (err.request) {
        setError('ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.');
      } else {
        setError(`ì„¸ì…˜ì„ ì´ˆê¸°í™”í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: ${err.message}`);
      }
    } finally {
      setLoading(false);
    }
  };

  const generateInitialQuestion = async (currentThreadId) => {
    try {
      const response = await aiInterviewAPI.chat('ì•ˆë…•í•˜ì„¸ìš”. ë©´ì ‘ì„ ì‹œì‘í•´ì£¼ì„¸ìš”.', currentThreadId);
      
      if (response.success) {
        setAiResponse(response.response);
        setCurrentQuestion({
          question_text: response.response,
          question_type: 'ì¸ì„±',
          question_number: 1
        });
        
        // AI ìŒì„± ì¬ìƒ
        if (response.audioContent) {
          const audioBlob = new Blob(
            [Uint8Array.from(atob(response.audioContent), c => c.charCodeAt(0))],
            { type: 'audio/mp3' }
          );
          const audioUrl = URL.createObjectURL(audioBlob);
          setAiAudioUrl(audioUrl);
          
          const audio = new Audio(audioUrl);
          audio.play();
        }
      } else {
        throw new Error(response.error || 'ì´ˆê¸° ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
      }
    } catch (err) {
      console.error('Error generating initial question:', err);
      
      // ë” êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€ ì œê³µ
      if (err.response) {
        const errorMessage = err.response.data?.error || err.response.data?.message || 'ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.';
        setError(`ì´ˆê¸° ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: ${errorMessage}`);
      } else if (err.request) {
        setError('ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
      } else {
        setError(`ì´ˆê¸° ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: ${err.message}`);
      }
    }
  };

  const initializeSpeechRecognition = () => {
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      recognitionRef.current = new SpeechRecognition();
      recognitionRef.current.continuous = true;
      recognitionRef.current.interimResults = true;
      recognitionRef.current.lang = 'ko-KR';

      recognitionRef.current.onresult = (event) => {
        let finalTranscript = '';
        let interimTranscript = '';

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript;
          } else {
            interimTranscript += transcript;
          }
        }

        setTranscription(finalTranscript + interimTranscript);
      };

      recognitionRef.current.onerror = (event) => {
        console.error('Speech recognition error:', event.error);
        setError('ìŒì„± ì¸ì‹ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
      };
    } else {
      setError('ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
    }
  };

  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRecorderRef.current = new MediaRecorder(stream);
      audioChunksRef.current = [];

      mediaRecorderRef.current.ondataavailable = (event) => {
        audioChunksRef.current.push(event.data);
      };

      mediaRecorderRef.current.onstop = () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/wav' });
        handleAnswerSubmission(audioBlob);
      };

      mediaRecorderRef.current.start();
      setIsRecording(true);
      setTranscription('');
      
      // ìŒì„± ì¸ì‹ ì‹œì‘
      if (recognitionRef.current) {
        recognitionRef.current.start();
      }
    } catch (err) {
      setError('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.');
      console.error('Error starting recording:', err);
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
      
      // ìŒì„± ì¸ì‹ ì¤‘ì§€
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
    }
  };

  const handleAnswerSubmission = async (audioBlob) => {
    try {
      setIsProcessing(true);
      setError(null);

      console.log('=== ë‹µë³€ ì²˜ë¦¬ ì‹œì‘ ===');
      console.log('Thread ID:', threadId);
      console.log('ì˜¤ë””ì˜¤ Blob í¬ê¸°:', audioBlob.size, 'bytes');

      // ìŒì„± ì¸ì‹ (STT)
      console.log('ìŒì„± ì¸ì‹ ìš”ì²­ ì¤‘...');
      const transcribeResponse = await aiInterviewAPI.transcribeAudio(audioBlob);
      console.log('ìŒì„± ì¸ì‹ ì‘ë‹µ:', transcribeResponse);
      
      if (!transcribeResponse.success) {
        throw new Error(transcribeResponse.error || 'ìŒì„± ì¸ì‹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
      }

      const userTranscription = transcribeResponse.transcription;
      console.log('=== ì‚¬ìš©ì ìŒì„± ì¸ì‹ ê²°ê³¼ ===');
      console.log('ì‚¬ìš©ì ì‘ë‹µ:', userTranscription);
      setTranscription(userTranscription);

      // AIì™€ ëŒ€í™”
      console.log('AI ì±„íŒ… ìš”ì²­ ì¤‘...');
      const chatResponse = await aiInterviewAPI.chat(userTranscription, threadId);
      console.log('AI ì±„íŒ… ì‘ë‹µ:', chatResponse);
      
      if (chatResponse.success) {
        console.log('=== AI ì‘ë‹µ ===');
        console.log('AI ì‘ë‹µ:', chatResponse.response);
        setAiResponse(chatResponse.response);
        
        // AI ìŒì„± ì¬ìƒ
        if (chatResponse.audioContent) {
          console.log('AI ìŒì„± ì¬ìƒ ì‹œì‘...');
          const audioBlob = new Blob(
            [Uint8Array.from(atob(chatResponse.audioContent), c => c.charCodeAt(0))],
            { type: 'audio/mp3' }
          );
          const audioUrl = URL.createObjectURL(audioBlob);
          setAiAudioUrl(audioUrl);
          
          const audio = new Audio(audioUrl);
          audio.play().catch(audioError => {
            console.error('AI ìŒì„± ì¬ìƒ ì‹¤íŒ¨:', audioError);
          });
        }

        // ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì—…ë°ì´íŠ¸ (question_textë§Œ ì‚¬ìš©)
        setCurrentQuestion({
          question_text: chatResponse.response,
          question_type: 'AI ì§ˆë¬¸',
          question_number: (currentQuestion?.question_number || 0) + 1
        });
        
        console.log('=== ë‹µë³€ ì²˜ë¦¬ ì™„ë£Œ ===');
      } else {
        throw new Error(chatResponse.error || 'AI ì‘ë‹µì„ ë°›ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
      }
    } catch (err) {
      console.error('=== ë‹µë³€ ì²˜ë¦¬ ì˜¤ë¥˜ ===');
      console.error('ì˜¤ë¥˜ íƒ€ì…:', err.constructor.name);
      console.error('ì˜¤ë¥˜ ë©”ì‹œì§€:', err.message);
      console.error('ì˜¤ë¥˜ ìŠ¤íƒ:', err.stack);
      
      // ë” êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€ ì œê³µ
      if (err.response) {
        console.error('ì‘ë‹µ ìƒíƒœ:', err.response.status);
        console.error('ì‘ë‹µ ë°ì´í„°:', err.response.data);
        const errorMessage = err.response.data?.error || err.response.data?.message || 'ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.';
        setError(`ë‹µë³€ ì²˜ë¦¬ ì‹¤íŒ¨: ${errorMessage}`);
      } else if (err.request) {
        console.error('ìš”ì²­ ì˜¤ë¥˜:', err.request);
        setError('ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
      } else {
        setError(`ë‹µë³€ì„ ì²˜ë¦¬í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: ${err.message}`);
      }
    } finally {
      setIsProcessing(false);
    }
  };

  const handleEndInterview = async () => {
    if (window.confirm('ë©´ì ‘ì„ ì¢…ë£Œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?')) {
      try {
        await aiInterviewAPI.endSession(threadId);
        navigate('/ai-interview');
      } catch (err) {
        setError('ë©´ì ‘ì„ ì¢…ë£Œí•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
        console.error('Error ending session:', err);
      }
    }
  };

  const handlePauseInterview = () => {
    navigate('/ai-interview');
  };

  if (loading) {
    return (
      <div className="ai-interview-room-container">
        <div className="loading">ë©´ì ‘ì‹¤ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤...</div>
      </div>
    );
  }

  if (error && !session) {
    return (
      <div className="ai-interview-room-container">
        <div className="error">{error}</div>
        <button onClick={() => navigate('/ai-interview')} className="back-btn">
          ëª©ë¡ìœ¼ë¡œ ëŒì•„ê°€ê¸°
        </button>
      </div>
    );
  }

  return (
    <div className="ai-interview-room-container">
      <div className="interview-header">
        <div className="company-info">
          <h2>{session?.company?.name || 'ê¸°ì—…ëª… ì—†ìŒ'}</h2>
          <p>AI ë©´ì ‘ ì§„í–‰ ì¤‘</p>
        </div>
        
        <div className="session-info">
          <span className="question-counter">
            {session?.current_question_number || 0} / {session?.total_questions || 0}
          </span>
        </div>

        <div className="interview-controls">
          <button onClick={handlePauseInterview} className="pause-btn">
            ì¼ì‹œì •ì§€
          </button>
          <button onClick={handleEndInterview} className="end-btn">
            ë©´ì ‘ ì¢…ë£Œ
          </button>
        </div>
      </div>

      <div className="interview-content">
        {currentQuestion ? (
          <div className="question-section">
            <div className="question-card">
              <div className="question-header">
                <span className="question-type">{currentQuestion.question_type}</span>
                <span className="question-number">ì§ˆë¬¸ {currentQuestion.question_number}</span>
              </div>
              <h3 className="question-text">{currentQuestion.question_text}</h3>
            </div>
            
            {aiAudioUrl && (
              <div className="ai-audio-controls">
                <button 
                  onClick={() => {
                    const audio = new Audio(aiAudioUrl);
                    audio.play();
                  }}
                  className="play-ai-audio-btn"
                >
                  ğŸ”Š AI ìŒì„± ë‹¤ì‹œ ë“£ê¸°
                </button>
              </div>
            )}
          </div>
        ) : (
          <div className="no-question">
            <p>ì§ˆë¬¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...</p>
          </div>
        )}

        <div className="answer-section">
          <div className="transcription-area">
            <h4>ë‹µë³€ ë‚´ìš©</h4>
            <div className="transcription-text">
              {transcription || 'ìŒì„±ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”...'}
            </div>
          </div>

          <div className="recording-controls">
            {!isRecording && !isProcessing ? (
              <button onClick={startRecording} className="record-btn">
                ğŸ¤ ë‹µë³€ ì‹œì‘
              </button>
            ) : isRecording ? (
              <button onClick={stopRecording} className="stop-btn">
                â¹ï¸ ë‹µë³€ ì™„ë£Œ
              </button>
            ) : (
              <div className="processing">
                <div className="spinner"></div>
                <p>ë‹µë³€ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...</p>
              </div>
            )}
          </div>
        </div>
      </div>

      {error && (
        <div className="error-message">
          {error}
          <button onClick={() => setError(null)} className="close-error">
            âœ•
          </button>
        </div>
      )}

      <div className="interview-tips">
        <h4>ë©´ì ‘ íŒ</h4>
        <ul>
          <li>ëª…í™•í•˜ê³  ìì‹ ê° ìˆê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.</li>
          <li>êµ¬ì²´ì ì¸ ê²½í—˜ê³¼ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ì„¸ìš”.</li>
          <li>ë‹µë³€ í›„ "ë‹µë³€ ì™„ë£Œ" ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.</li>
          <li>ë©´ì ‘ ì¤‘ ì–¸ì œë“ ì§€ ì¼ì‹œì •ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
        </ul>
      </div>
    </div>
  );
};

export default AIInterviewRoom;
